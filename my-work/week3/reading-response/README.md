### Responding to Podcast with Virginia Eubanks on *AUTOMATING INEQUALITY*

* **Question 1: How do technical tools promise to "fair out" the remaining discrimination that exist in social/welfare systems? In how far can they succeed, in which ways do they fail?**

The question itself is already questionable, in that those tools do not make any promises. Or, at least promises are made by the governments and companies, who “owns” the technology. Surely, they have to be useful in order to be regarded as “tools,” but are they helpful? Eubanks in the podcast talked about how discrimination has never been a data-emittable problem. It is very eye-opening to think about how data has limits in reaching the pre-existing bias in society. Data succeeds by their ability to process a large amount of data with countless variables. However, we also know from the examples given that if there is a mistake made at any stage, there could be severe consequences due to the sequences of institutionalized structures.

* **Question 2: Imagine, what could this (following quotes) mean in the widest sense? "The state doesn't need a cop to kill a person" and "electronic incarceration"**

The widest and wildest scenes I can imagine do not only live in Black Mirror. From the literal reading, it is informing that a cop is no longer efficient in executing his/her mission. Generally, a cop patrols to check on an area, but technology monitors and collects data without public power and credibility, with no limited area. In the end, we all become metaphors, the state, a cop, a person, and “kill” means destroy in all ways possible. Worst case is, there would need no reasoning for destruction. We can never escape. Eubanks stated that there is hardly space to say no to the tools. Well, as normal citizens, we don’t choose the tools. The tools choose us. Disregarding “the cop’s” reasoning and accuracy, we as persons are to be killed should “the state” give us a red sign.

* **Question 3: What do you understand this to mean? "systems act as a kind of 'empathy-overwrite'"**

To be honest, everything is a word game. If the overwrite here means “replace,” then it could burden our naive hope that those systems (could be designed by biased people) would serve as the objective voice that defeats our biases. But If it means “written in a way that is not clear and simple,” the decisions these systems make are complicated by layers and layers of interventions until we reach that surface of “empathy.” Then, this “moral thermometer” could use its bias-infected standard to overwrite us.

* **Question 4: China is much more advanced and expansive when it comes to applying technical solutions to societal processes or instant challenges. Try to point out example cases in China that are in accordance or in opposition to the problematics discussed in the podcast. Perhaps you can think of "technical systems not well thought-through about what their impact on human beings is"**

We are aware of China’s enforcement on data collection, but the truth is, we know little about it. The most notable one would be the social credit system (perhaps Chinese are used to being graded since little…) The system seems to me as an extension applied to the pre-existing surveillance application. Not new. But it implies another value system encrypted in the social credit system, a system that may define “good” and “bad” differently from others. This is messed up. And the scale (covering the entirety of the nation) of this implementation would only enlarge this Party-led standard. It accords to what Eubanks stated, a lot of biases are enforced and reinforced through institutionalization and becomes structural bias.
